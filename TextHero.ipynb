{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texthero from zero to hero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Texthero** is a python package to let you work efficiently and quickly with text data. You can think of texhero as scikit learn for text-based dataset.\n",
    "\n",
    "**Overview**\n",
    "\n",
    "Given a dataset with structured data, it's easy to have a quick understanding of the underline data. Oppositely, given a dataset composed of text-only, it's harder to have a quick undertanding of the data. Texthero help you there, providing utility functions to quickly clean the text data, map it into a vector space and gather from it primary insights.\n",
    "\n",
    "**Pandas integration**\n",
    "\n",
    "One of the main pillar of texthero is that is designed from the ground-up to work with Pandas Dataframe and Series.\n",
    "\n",
    "Most of texthero methods, simply apply transformation to Pandas Series. As a rule of thumb, the first argument and the return ouputs of almost all texthero methods are either a Pandas Series or a Pandas DataFrame.\n",
    "\n",
    "**Pipeline**\n",
    "\n",
    "The first phase of almost any natural language processing is almost the same, independently to the specific task.\n",
    "\n",
    "Pandas introduced Pipe function starting from version 0.16.2. Pipe enables user-defined methods in method chains\n",
    "\n",
    "**Installation and import**\n",
    "\n",
    "Texthero is available on pip. To install it open a terminal and execute\n",
    "\n",
    "pip install texthero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already installed it and want to upgrade to the last version type:\n",
    "\n",
    "pip install texthero -U\n",
    "\n",
    "**Getting started**\n",
    "\n",
    "For a simple example we make use of the BBC Sport Dataset, it consists of 737 documents from the BBC Sport website corresponding to sports news articles in five topical areas from 2004-2005.\n",
    "\n",
    "The five different areas are athletics, cricket, football, rugby and tennis.\n",
    "\n",
    "The original dataset comes as a zip files with five different folder containing the article as text data for each topic.\n",
    "\n",
    "For convenience, we createdThis script simply read all text data and store it into a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import texthero and pandas.\n",
    "\n",
    "import texthero as hero\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the bbc sport dataset in a Pandas DataFrame.\n",
    "# df = pd.read_csv(\"https://github.com/jbesomi/texthero/raw/master/dataset/bbcsport.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "\n",
    "**Clean**\n",
    "\n",
    "To clean the text data all we have to do is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['clean_text'] = hero.clean(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips. \n",
    "When we need to define a new column returned from a function, we prepend the name of the function to the column name. Example:\n",
    "\n",
    "df['tsne_col'] = df['col'].pipe(hero.tsne). \n",
    "\n",
    "This keep the code simple to read and permit to construct complex pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default pipeline for the clean method is the following:\n",
    "\n",
    "- fillna(s) Replace not assigned values with empty spaces.\n",
    "- lowercase(s) Lowercase all text.\n",
    "- remove_digits() Remove all blocks of digits.\n",
    "- remove_punctuation() Remove all string.punctuation (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~).\n",
    "- remove_diacritics() Remove all accents from strings.\n",
    "- remove_stopwords() Remove all stop words.\n",
    "- remove_whitespace() Remove all white space between words.\n",
    "As texthero is still in beta, the default pipeline may undergo some minor changes in the next versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom pipeline**\n",
    "\n",
    "We can also pass a custom pipeline as argument to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from texthero import preprocessing\n",
    "\n",
    "# custom_pipeline = [preprocessing.fillna,\n",
    "#                    preprocessing.lowercase,\n",
    "#                    preprocessing.remove_whitespace]\n",
    "# df['clean_text'] = hero.clean(df['text'], custom_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively\n",
    "# df['clean_text'] = df['clean_text'].pipe(hero.clean, custom_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing API**\n",
    "\n",
    "The complete preprocessing API can be found at the following address: api preprocessing.\n",
    "\n",
    "**Representation**\n",
    "\n",
    "Once cleaned the data, the next natural is to map each document into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF representation\n",
    "# df['tfidf_clean_text'] = hero.tfidf(df['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality reduction with PCA**\n",
    "\n",
    "To visualize the data, we map each point to a two-dimensional representation with PCA. The principal component analysis algorithms returns the combination of attributes that better account the variance in the data.\n",
    "\n",
    "df['pca_tfidf_clean_text'] = hero.pca(df['tfidf_clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All in one step**\n",
    "\n",
    "We can achieve all the three steps show above, cleaning, tf-idf representation and dimensionality reduction in a single step. Isn't fabulous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['pca'] = (\n",
    "#             df['text']\n",
    "#             .pipe(hero.clean)\n",
    "#             .pipe(hero.tfidf)\n",
    "#             .pipe(hero.pca)\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representation API**\n",
    "\n",
    "The complete representation module API can be found at the following address: api representation.\n",
    "\n",
    "**Visualization**\n",
    "\n",
    "texthero.visualization provide some helpers functions to visualize the transformed Dataframe. All visualization utilize under the hoods the Plotly Python Open Source Graphing Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hero.scatterplot(df, col='pca', color='topic', title=\"PCA BBC Sport news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can \"visualize\" the most common words for each topic with top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_TOP_WORDS = 5\n",
    "# df.groupby('topic')['text'].apply(lambda x: hero.top_words(x)[:NUM_TOP_WORDS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization API**\n",
    "\n",
    "The complete visualization module API can be found at the following address: api visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We saw how in just a couple of lines of code we can represent and visualize any text dataset. We went from knowing nothing regarding the dataset to see that there are 5 (quite) distinct areas representig each topic. We went from zero to hero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import texthero as hero\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://github.com/jbesomi/texthero/raw/master/dataset/bbcsport.csv\"\n",
    ")\n",
    "df['pca'] = (\n",
    "    df['text']\n",
    "    .pipe(hero.clean)\n",
    "    .pipe(hero.tfidf)\n",
    "    .pipe(hero.pca)\n",
    ")\n",
    "\n",
    "hero.scatterplot(df, col='pca', color='topic', title=\"PCA BBC Sport news\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
